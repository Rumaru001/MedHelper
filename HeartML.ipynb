{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "focused-qualification",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\n1. age: age in years \\n2. sex: sex (1 = male; 0 = female)\\n3. cp: chest pain type \\n        -- Value 1: typical angina \\n        -- Value 2: atypical angina \\n        -- Value 3: non-anginal pain \\n        -- Value 4: asymptomatic \\n4. trestbps: resting blood pressure (in mm Hg on admission to the hospital) \\n5. chol: serum cholestoral in mg/dl \\n6. fbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) \\n7. restecg: resting electrocardiographic results \\n        -- Value 0: normal \\n        -- Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) \\n        -- Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria \\n8. thalach: maximum heart rate achieved  \\n9. exang: exercise induced angina (1 = yes; 0 = no) \\n10. oldpeak = ST depression induced by exercise relative to rest \\n11. slope: the slope of the peak exercise ST segment \\n        -- Value 1: upsloping \\n        -- Value 2: flat \\n        -- Value 3: downsloping \\n12. ca: number of major vessels (0-3) colored by flourosopy \\n13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect \\n\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "'''\n",
    "1. age: age in years \n",
    "2. sex: sex (1 = male; 0 = female)\n",
    "3. cp: chest pain type \n",
    "        -- Value 1: typical angina \n",
    "        -- Value 2: atypical angina \n",
    "        -- Value 3: non-anginal pain \n",
    "        -- Value 4: asymptomatic \n",
    "4. trestbps: resting blood pressure (in mm Hg on admission to the hospital) \n",
    "5. chol: serum cholestoral in mg/dl \n",
    "6. fbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) \n",
    "7. restecg: resting electrocardiographic results \n",
    "        -- Value 0: normal \n",
    "        -- Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) \n",
    "        -- Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria \n",
    "8. thalach: maximum heart rate achieved  \n",
    "9. exang: exercise induced angina (1 = yes; 0 = no) \n",
    "10. oldpeak = ST depression induced by exercise relative to rest \n",
    "11. slope: the slope of the peak exercise ST segment \n",
    "        -- Value 1: upsloping \n",
    "        -- Value 2: flat \n",
    "        -- Value 3: downsloping \n",
    "12. ca: number of major vessels (0-3) colored by flourosopy \n",
    "13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "wanted-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0     63    1   3       145   233    1        0      150      0      2.3   \n",
       "1     37    1   2       130   250    0        1      187      0      3.5   \n",
       "2     41    0   1       130   204    0        0      172      0      1.4   \n",
       "3     56    1   1       120   236    0        1      178      0      0.8   \n",
       "4     57    0   0       120   354    0        1      163      1      0.6   \n",
       "..   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...   \n",
       "298   57    0   0       140   241    0        1      123      1      0.2   \n",
       "299   45    1   3       110   264    0        1      132      0      1.2   \n",
       "300   68    1   0       144   193    1        1      141      0      3.4   \n",
       "301   57    1   0       130   131    0        1      115      1      1.2   \n",
       "302   57    0   1       130   236    0        0      174      0      0.0   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "0        0   0     1       1  \n",
       "1        0   0     2       1  \n",
       "2        2   0     2       1  \n",
       "3        2   0     2       1  \n",
       "4        2   0     2       1  \n",
       "..     ...  ..   ...     ...  \n",
       "298      1   0     3       0  \n",
       "299      1   0     3       0  \n",
       "300      1   2     3       0  \n",
       "301      1   1     3       0  \n",
       "302      1   1     2       0  \n",
       "\n",
       "[303 rows x 14 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>sex</th>\n      <th>cp</th>\n      <th>trestbps</th>\n      <th>chol</th>\n      <th>fbs</th>\n      <th>restecg</th>\n      <th>thalach</th>\n      <th>exang</th>\n      <th>oldpeak</th>\n      <th>slope</th>\n      <th>ca</th>\n      <th>thal</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>63</td>\n      <td>1</td>\n      <td>3</td>\n      <td>145</td>\n      <td>233</td>\n      <td>1</td>\n      <td>0</td>\n      <td>150</td>\n      <td>0</td>\n      <td>2.3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>37</td>\n      <td>1</td>\n      <td>2</td>\n      <td>130</td>\n      <td>250</td>\n      <td>0</td>\n      <td>1</td>\n      <td>187</td>\n      <td>0</td>\n      <td>3.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>41</td>\n      <td>0</td>\n      <td>1</td>\n      <td>130</td>\n      <td>204</td>\n      <td>0</td>\n      <td>0</td>\n      <td>172</td>\n      <td>0</td>\n      <td>1.4</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>56</td>\n      <td>1</td>\n      <td>1</td>\n      <td>120</td>\n      <td>236</td>\n      <td>0</td>\n      <td>1</td>\n      <td>178</td>\n      <td>0</td>\n      <td>0.8</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>57</td>\n      <td>0</td>\n      <td>0</td>\n      <td>120</td>\n      <td>354</td>\n      <td>0</td>\n      <td>1</td>\n      <td>163</td>\n      <td>1</td>\n      <td>0.6</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>298</th>\n      <td>57</td>\n      <td>0</td>\n      <td>0</td>\n      <td>140</td>\n      <td>241</td>\n      <td>0</td>\n      <td>1</td>\n      <td>123</td>\n      <td>1</td>\n      <td>0.2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>299</th>\n      <td>45</td>\n      <td>1</td>\n      <td>3</td>\n      <td>110</td>\n      <td>264</td>\n      <td>0</td>\n      <td>1</td>\n      <td>132</td>\n      <td>0</td>\n      <td>1.2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>68</td>\n      <td>1</td>\n      <td>0</td>\n      <td>144</td>\n      <td>193</td>\n      <td>1</td>\n      <td>1</td>\n      <td>141</td>\n      <td>0</td>\n      <td>3.4</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>301</th>\n      <td>57</td>\n      <td>1</td>\n      <td>0</td>\n      <td>130</td>\n      <td>131</td>\n      <td>0</td>\n      <td>1</td>\n      <td>115</td>\n      <td>1</td>\n      <td>1.2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>302</th>\n      <td>57</td>\n      <td>0</td>\n      <td>1</td>\n      <td>130</td>\n      <td>236</td>\n      <td>0</td>\n      <td>0</td>\n      <td>174</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>303 rows Ã— 14 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = 0\n",
    "\n",
    "X, y = df.drop(columns=['target']), df.loc[:, 'target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, recall_score, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_depth\":[3, 4, 5],\n",
    "    \"n_estimators\":[60, 70, 50, 40]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(RandomForestClassifier(random_state=random_state), param_grid=params,cv=3,n_jobs=-1, scoring=\"recall\", ).fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(clf, X_train, X_test, y_test=y_test):\n",
    "    print(\"TRAIN\",clf.score(X_train,y_train))\n",
    "    print(\"TEST\",clf.score(X_test,y_test))\n",
    "    # print(clf.best_params_)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"AUC\",roc_auc_score(y_test, y_pred))\n",
    "    print(\"RECALL\",recall_score(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "#stats(grid, X_train_scaled, X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('ca', 0.14237629813281696)\n('thal', 0.13282729861186368)\n('oldpeak', 0.13251528910778326)\n('cp', 0.11051004601643066)\n('thalach', 0.1030216129165023)\n('exang', 0.08784621295776358)\n('trestbps', 0.06097909314300931)\n('chol', 0.06047266566285679)\n('sex', 0.056787044018219775)\n('slope', 0.052320488868388366)\n('age', 0.041812459056067744)\n('restecg', 0.011144289583806154)\n('fbs', 0.007387201924491325)\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(random_state=random_state,max_depth= 5, n_estimators=40).fit(X_train_scaled, y_train)\n",
    "# stats(classifier, X_train_scaled, X_test_scaled)\n",
    "\n",
    "for v in sorted(list(zip(X.columns, classifier.feature_importances_)), key= lambda x: -x[1]):\n",
    "    print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9426229508196722\n0.9302325581395349\n{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 10}\n0.8932926829268292\n[[23 10]\n [ 3 40]]\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"learning_rate\":[ 0.1, 1, 0.5],\n",
    "    \"max_depth\":[2],\n",
    "    \"n_estimators\":[10,50]\n",
    "}\n",
    "\n",
    "random_clf = GridSearchCV(GradientBoostingClassifier(random_state=random_state), param_grid=params,cv=3,n_jobs=-1, scoring=\"recall\").fit(X_train_scaled, y_train)\n",
    "\n",
    "stats(random_clf, X_train_scaled, X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('cp', 0.3669285424474521)\n('ca', 0.2325037315911726)\n('thal', 0.22510508859981584)\n('oldpeak', 0.07045607584546569)\n('exang', 0.0363000283288606)\n('thalach', 0.03490177534447)\n('sex', 0.033804757842763065)\n('age', 0.0)\n('trestbps', 0.0)\n('chol', 0.0)\n('fbs', 0.0)\n('restecg', 0.0)\n('slope', 0.0)\n"
     ]
    }
   ],
   "source": [
    "classifier = GradientBoostingClassifier(max_depth= 2, n_estimators=10, learning_rate=0.1).fit(X_train_scaled, y_train)\n",
    "# stats(classifier, X_train_scaled, X_test_scaled)\n",
    "\n",
    "for v in sorted(list(zip(X.columns, classifier.feature_importances_)), key= lambda x: -x[1]):\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n 5 0.1\nTRAIN 0.801762114537445\nTEST 0.7631578947368421\nAUC 0.7519379844961241\nRECALL 0.8372093023255814\n[[22 11]\n [ 7 36]]\n\n 5 0.3\nTRAIN 0.788546255506608\nTEST 0.7631578947368421\nAUC 0.7519379844961241\nRECALL 0.8372093023255814\n[[22 11]\n [ 7 36]]\n\n 10 0.1\nTRAIN 0.7929515418502202\nTEST 0.7894736842105263\nAUC 0.7822410147991544\nRECALL 0.8372093023255814\n[[24  9]\n [ 7 36]]\n\n 10 0.3\nTRAIN 0.7973568281938326\nTEST 0.7631578947368421\nAUC 0.7519379844961241\nRECALL 0.8372093023255814\n[[22 11]\n [ 7 36]]\n\n 15 0.1\nTRAIN 0.7929515418502202\nTEST 0.7763157894736842\nAUC 0.7670894996476393\nRECALL 0.8372093023255814\n[[23 10]\n [ 7 36]]\n\n 15 0.3\nTRAIN 0.801762114537445\nTEST 0.75\nAUC 0.7403100775193799\nRECALL 0.813953488372093\n[[22 11]\n [ 8 35]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# params = {\n",
    "#     \"gamma\":[0.1, 0.3, ],\n",
    "#     \"C\":[0.1, 0.5, 1, 2, 3],\n",
    "#     \"degree\":[2,3]\n",
    "# }\n",
    "\n",
    "# svc_clf = GridSearchCV(SVC(random_state=random_state), param_grid=params,cv=5,n_jobs=-1, scoring=\"recall\").fit(X_train_scaled, y_train)\n",
    "# stats(svc_clf, X_train_scaled, X_test_scaled)\n",
    "for c in [5, 10, 15]: \n",
    "    for g in[0.1, 0.3]:\n",
    "        res = SVC(random_state=random_state, C=c, gamma=g).fit(X_train_scaled, y_train)\n",
    "        print('\\n',c,g)\n",
    "        stats(res, X_train_scaled, X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.01213121, 0.00975895, 0.03023944, 0.0499527 , 0.01895947,\n",
       "        0.00646739, 0.015766  , 0.00945015, 0.00953155, 0.00626922,\n",
       "        0.00638318, 0.00405846, 0.00586791, 0.00358458, 0.00464287,\n",
       "        0.00523076, 0.00476923, 0.00640683, 0.00783429, 0.005862  ]),\n",
       " 'std_fit_time': array([0.00668342, 0.00324633, 0.0420394 , 0.03366899, 0.01607006,\n",
       "        0.00188859, 0.01258104, 0.00689702, 0.0038314 , 0.00332274,\n",
       "        0.00324205, 0.00055382, 0.00206949, 0.00074781, 0.00201183,\n",
       "        0.0030918 , 0.00336307, 0.00339509, 0.00375151, 0.00289012]),\n",
       " 'mean_score_time': array([0.00739617, 0.0189003 , 0.065657  , 0.0559267 , 0.0099503 ,\n",
       "        0.00764875, 0.01173735, 0.00647202, 0.00569162, 0.00361409,\n",
       "        0.00602031, 0.00589414, 0.00428009, 0.0054276 , 0.00472236,\n",
       "        0.00453582, 0.00444264, 0.00617123, 0.00500908, 0.00359941]),\n",
       " 'std_score_time': array([0.00266159, 0.02265772, 0.07145332, 0.04586347, 0.00533791,\n",
       "        0.00313982, 0.01032396, 0.00240732, 0.00253073, 0.00077223,\n",
       "        0.00276461, 0.00297433, 0.00113526, 0.00277629, 0.00208872,\n",
       "        0.00300747, 0.00178787, 0.00337524, 0.00331149, 0.00129333]),\n",
       " 'param_C': masked_array(data=[0.1, 0.1, 0.1, 0.1, 0.5, 0.5, 0.5, 0.5, 1, 1, 1, 1, 2,\n",
       "                    2, 2, 2, 3, 3, 3, 3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_degree': masked_array(data=[2, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3, 2, 2,\n",
       "                    3, 3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_gamma': masked_array(data=[0.1, 0.3, 0.1, 0.3, 0.1, 0.3, 0.1, 0.3, 0.1, 0.3, 0.1,\n",
       "                    0.3, 0.1, 0.3, 0.1, 0.3, 0.1, 0.3, 0.1, 0.3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 0.1, 'degree': 2, 'gamma': 0.1},\n",
       "  {'C': 0.1, 'degree': 2, 'gamma': 0.3},\n",
       "  {'C': 0.1, 'degree': 3, 'gamma': 0.1},\n",
       "  {'C': 0.1, 'degree': 3, 'gamma': 0.3},\n",
       "  {'C': 0.5, 'degree': 2, 'gamma': 0.1},\n",
       "  {'C': 0.5, 'degree': 2, 'gamma': 0.3},\n",
       "  {'C': 0.5, 'degree': 3, 'gamma': 0.1},\n",
       "  {'C': 0.5, 'degree': 3, 'gamma': 0.3},\n",
       "  {'C': 1, 'degree': 2, 'gamma': 0.1},\n",
       "  {'C': 1, 'degree': 2, 'gamma': 0.3},\n",
       "  {'C': 1, 'degree': 3, 'gamma': 0.1},\n",
       "  {'C': 1, 'degree': 3, 'gamma': 0.3},\n",
       "  {'C': 2, 'degree': 2, 'gamma': 0.1},\n",
       "  {'C': 2, 'degree': 2, 'gamma': 0.3},\n",
       "  {'C': 2, 'degree': 3, 'gamma': 0.1},\n",
       "  {'C': 2, 'degree': 3, 'gamma': 0.3},\n",
       "  {'C': 3, 'degree': 2, 'gamma': 0.1},\n",
       "  {'C': 3, 'degree': 2, 'gamma': 0.3},\n",
       "  {'C': 3, 'degree': 3, 'gamma': 0.1},\n",
       "  {'C': 3, 'degree': 3, 'gamma': 0.3}],\n",
       " 'split0_test_score': array([0.88, 0.84, 0.88, 0.84, 0.84, 0.8 , 0.84, 0.8 , 0.84, 0.84, 0.84,\n",
       "        0.84, 0.8 , 0.8 , 0.8 , 0.8 , 0.84, 0.76, 0.84, 0.76]),\n",
       " 'split1_test_score': array([0.96, 0.84, 0.96, 0.84, 0.84, 0.8 , 0.84, 0.8 , 0.8 , 0.8 , 0.8 ,\n",
       "        0.8 , 0.84, 0.8 , 0.84, 0.8 , 0.8 , 0.8 , 0.8 , 0.8 ]),\n",
       " 'split2_test_score': array([0.91666667, 0.875     , 0.91666667, 0.875     , 0.875     ,\n",
       "        0.91666667, 0.875     , 0.91666667, 0.91666667, 0.875     ,\n",
       "        0.91666667, 0.875     , 0.875     , 0.91666667, 0.875     ,\n",
       "        0.91666667, 0.875     , 0.875     , 0.875     , 0.875     ]),\n",
       " 'split3_test_score': array([0.91666667, 0.83333333, 0.91666667, 0.83333333, 0.83333333,\n",
       "        0.83333333, 0.83333333, 0.83333333, 0.83333333, 0.83333333,\n",
       "        0.83333333, 0.83333333, 0.79166667, 0.79166667, 0.79166667,\n",
       "        0.79166667, 0.79166667, 0.83333333, 0.79166667, 0.83333333]),\n",
       " 'split4_test_score': array([1.        , 0.95833333, 1.        , 0.95833333, 0.95833333,\n",
       "        0.95833333, 0.95833333, 0.95833333, 0.95833333, 0.91666667,\n",
       "        0.95833333, 0.91666667, 0.91666667, 0.95833333, 0.91666667,\n",
       "        0.95833333, 0.91666667, 0.95833333, 0.91666667, 0.95833333]),\n",
       " 'mean_test_score': array([0.93466667, 0.86933333, 0.93466667, 0.86933333, 0.86933333,\n",
       "        0.86166667, 0.86933333, 0.86166667, 0.86966667, 0.853     ,\n",
       "        0.86966667, 0.853     , 0.84466667, 0.85333333, 0.84466667,\n",
       "        0.85333333, 0.84466667, 0.84533333, 0.84466667, 0.84533333]),\n",
       " 'std_test_score': array([0.04134408, 0.04684015, 0.04134408, 0.04684015, 0.04684015,\n",
       "        0.0644636 , 0.04684015, 0.0644636 , 0.05852255, 0.0397548 ,\n",
       "        0.05852255, 0.0397548 , 0.04675468, 0.07003967, 0.04675468,\n",
       "        0.07003967, 0.04675468, 0.0680147 , 0.04675468, 0.0680147 ]),\n",
       " 'rank_test_score': array([ 1,  5,  1,  5,  5,  9,  5,  9,  3, 13,  3, 13, 17, 11, 17, 11, 17,\n",
       "        15, 17, 15], dtype=int32)}"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "svc_clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9180327868852459\n0.9069767441860465\n{'C': 0.1, 'penalty': 'l2', 'solver': 'saga'}\n0.8939999999999999\n[[23 10]\n [ 4 39]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "params = {\n",
    "    \"penalty\":['l1', 'l2'],\n",
    "    \"C\":[0.1, 0.5, 1, 2, 5, 10],\n",
    "    \"solver\":['liblinear' , 'saga']\n",
    "}\n",
    "\n",
    "poly = PolynomialFeatures().fit(X_train_scaled)\n",
    "X_train_scaled_poly = poly.transform(X_train_scaled)\n",
    "X_test_scaled_poly = poly.transform(X_test_scaled)\n",
    "\n",
    "lin_reg_clf = GridSearchCV(LogisticRegression(random_state=random_state), param_grid=params,cv=5,n_jobs=-1, scoring=\"recall\").fit(X_train_scaled_poly, y_train)\n",
    "stats(lin_reg_clf, X_train_scaled_poly, X_test_scaled_poly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8934426229508197\n0.8837209302325582\n{'activation': 'logistic', 'hidden_layer_sizes': (500, 1500, 500, 60)}\n0.9359999999999999\n[[24  9]\n [ 5 38]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "params = {\n",
    "    \"activation\":['logistic'],\n",
    "    \"hidden_layer_sizes\":[(500,1000,500,50),(100,1000,200),(500,1500,500,60),(100,500,70)]\n",
    "}\n",
    "\n",
    "neural = GridSearchCV(MLPClassifier(random_state=random_state), param_grid=params,cv=5,n_jobs=-1, scoring=\"recall\").fit(X_train_scaled_poly, y_train)\n",
    "\n",
    "stats(neural, X_train_scaled_poly, X_test_scaled_poly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, recall_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, ['cp','exang','thalach','sex','age','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " RandomForestClassifier\n",
      "TRAIN 0.8852459016393442\n",
      "TEST 0.813953488372093\n",
      "{'max_depth': 3, 'n_estimators': 40}\n",
      "AUC 0.7251585623678647\n",
      "RECALL 0.813953488372093\n",
      "[[21 12]\n",
      " [ 8 35]]\n",
      "\n",
      " GradientBoostingClassifier\n",
      "TRAIN 0.8032786885245902\n",
      "TEST 0.7209302325581395\n",
      "{'learning_rate': 0.5, 'max_depth': 2, 'n_estimators': 10}\n",
      "AUC 0.7089499647639184\n",
      "RECALL 0.7209302325581395\n",
      "[[23 10]\n",
      " [12 31]]\n",
      "\n",
      " SVC\n",
      "TRAIN 0.8770491803278688\n",
      "TEST 0.9302325581395349\n",
      "{'C': 0.1, 'degree': 2, 'gamma': 0.1}\n",
      "AUC 0.7075405214940098\n",
      "RECALL 0.9302325581395349\n",
      "[[16 17]\n",
      " [ 3 40]]\n",
      "\n",
      " MLPClassifier\n",
      "TRAIN 1.0\n",
      "TEST 1.0\n",
      "{'activation': 'logistic', 'hidden_layer_sizes': (100, 150, 50)}\n",
      "AUC 0.5\n",
      "RECALL 1.0\n",
      "[[ 0 33]\n",
      " [ 0 43]]\n",
      "\n",
      " LogisticRegression\n",
      "TRAIN 0.860655737704918\n",
      "TEST 0.9069767441860465\n",
      "{'C': 0.1, 'penalty': 'l1', 'solver': 'saga'}\n",
      "AUC 0.6959126145172656\n",
      "RECALL 0.9069767441860465\n",
      "[[16 17]\n",
      " [ 4 39]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "df = df.loc[:, ['cp','exang','thalach','sex','age','target']]\n",
    "\n",
    "random_state = 0\n",
    "\n",
    "X, y = df.drop(columns=['target']), df.loc[:, 'target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=random_state)\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "params = [{\n",
    "    \"max_depth\":[3, 4, 5],\n",
    "    \"n_estimators\":[60, 70, 50, 40]\n",
    "},\n",
    "{\n",
    "    \"learning_rate\":[ 0.1, 1, 0.5],\n",
    "    \"max_depth\":[2],\n",
    "    \"n_estimators\":[10,50]\n",
    "},{\n",
    "    \"gamma\":[0.1, 0.5, 1, 2, 5, 10],\n",
    "    \"C\":[0.1, 0.5, 1, 2, 5, 10],\n",
    "    \"degree\":[2,3,6,9]\n",
    "},{\n",
    "    \"activation\":['logistic'],\n",
    "    \"hidden_layer_sizes\":[(100,50),(100,100),(200,50),(100,150,50),(100)]\n",
    "},{\n",
    "    \"penalty\":['l1', 'l2'],\n",
    "    \"C\":[0.1, 0.5, 1, 2, 5, 10],\n",
    "    \"solver\":['liblinear' , 'saga']\n",
    "}]\n",
    "clfs = [RandomForestClassifier,GradientBoostingClassifier,SVC,MLPClassifier,LogisticRegression]\n",
    "\n",
    "for clf, prms in zip(clfs, params):\n",
    "    if clf == LogisticRegression:\n",
    "        poly = PolynomialFeatures().fit(X_train_scaled)\n",
    "        X_train_scaled = poly.transform(X_train_scaled)\n",
    "        X_test_scaled = poly.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "    grid = GridSearchCV(clf(random_state=random_state), param_grid=prms,cv=5,n_jobs=-1, scoring=\"recall\").fit(X_train_scaled, y_train)\n",
    "\n",
    "    print('\\n',clf.__name__)\n",
    "\n",
    "    stats(grid, X_train_scaled, X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['cp','ca','thal','oldpeak','exang','thalach','sex','age','target'] - RandomForest, SVC,  Logistic, \n",
    "\n",
    " RandomForestClassifier\n",
    "TRAIN 0.983216237314598\n",
    "TEST 0.9154334038054968\n",
    "{'max_depth': 5, 'n_estimators': 70}\n",
    "AUC 0.828752642706131\n",
    "RECALL 0.9302325581395349\n",
    "[[24  9]\n",
    " [ 3 40]]\n",
    "\n",
    "\n",
    " SVC\n",
    "TRAIN 0.950975800156128\n",
    "TEST 0.8992248062015503\n",
    "{'C': 5, 'degree': 2, 'gamma': 0.5}\n",
    "AUC 0.832276250880902\n",
    "RECALL 0.9069767441860465\n",
    "[[25  8]\n",
    " [ 4 39]]\n",
    "\n",
    "\n",
    " LogisticRegression\n",
    "TRAIN 0.9549570647931304\n",
    "TEST 0.8886539816772374\n",
    "{'C': 5, 'penalty': 'l1', 'solver': 'liblinear'}\n",
    "AUC 0.835799859055673\n",
    "RECALL 0.8837209302325582\n",
    "[[26  7]\n",
    " [ 5 38]]\n",
    "\n",
    " ++++++++++++++++++==++++++++==++++++++*+++++==++++++*++++++++==++++++++++==++++++++++++++++++++++\n",
    "\n",
    " ['cp','exang','thalach','sex','age','target'] - GradientBoostingClassifier,  SVC, MLPClassifier\n",
    "\n",
    " GradientBoostingClassifier\n",
    "TRAIN 0.8956674473067915\n",
    "TEST 0.835799859055673\n",
    "{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 10}\n",
    "AUC 0.7822410147991544\n",
    "RECALL 0.8372093023255814\n",
    "[[24  9]\n",
    " [ 7 36]]\n",
    "\n",
    " SVC\n",
    "TRAIN 0.877751756440281\n",
    "TEST 0.8266384778012684\n",
    "{'C': 10, 'degree': 2, 'gamma': 0.1} <<<<<++++++++++++++++\n",
    "AUC 0.7822410147991544\n",
    "RECALL 0.8372093023255814\n",
    "[[24  9]\n",
    " [ 7 36]]\n",
    "\n",
    " MLPClassifier\n",
    "TRAIN 0.8658079625292741\n",
    "TEST 0.7935165609584215\n",
    "{'activation': 'logistic', 'hidden_layer_sizes': (100, 50)}\n",
    "AUC 0.758985200845666\n",
    "RECALL 0.7906976744186046\n",
    "[[24  9]\n",
    " [ 9 34]]\n",
    "\n",
    "  ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "  ['cp','thal','exang','thalach','sex','age','target'] - MLPClassifier, RandomForestClassifier, SVC\n",
    "\n",
    "RandomForestClassifier\n",
    "TRAIN 0.9493364558938329\n",
    "TEST 0.8784355179704016\n",
    "{'max_depth': 4, 'n_estimators': 70}\n",
    "AUC 0.8090204369274137\n",
    "RECALL 0.8604651162790697\n",
    "[[25  8]\n",
    " [ 6 37]]\n",
    "\n",
    " SVC\n",
    "TRAIN 0.9007025761124122\n",
    "TEST 0.8689217758985202\n",
    "{'C': 2, 'degree': 2, 'gamma': 0.5}\n",
    "AUC 0.7973925299506696\n",
    "RECALL 0.8372093023255814\n",
    "[[25  8]\n",
    " [ 7 36]]\n",
    "\n",
    " MLPClassifier\n",
    "TRAIN 0.8755659640905542\n",
    "TEST 0.8231148696264976\n",
    "{'activation': 'logistic', 'hidden_layer_sizes': (100, 100)}\n",
    "AUC 0.8171247357293868\n",
    "RECALL 0.9069767441860465\n",
    "[[24  9]\n",
    " [ 4 39]]\n",
    "\n",
    "\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "['cp','ca','exang','thalach','sex','age','target'] - LogisticRegression,  GradientBoostingClassifier, SVC\n",
    "\n",
    "\n",
    " GradientBoostingClassifier\n",
    "TRAIN 0.9450819672131148\n",
    "TEST 0.8393234672304439\n",
    "{'learning_rate': 0.5, 'max_depth': 2, 'n_estimators': 10}\n",
    "AUC 0.7938689217758985\n",
    "RECALL 0.8604651162790697\n",
    "[[24  9]\n",
    " [ 6 37]]\n",
    "\n",
    " SVC\n",
    "TRAIN 0.9450429352068697\n",
    "TEST 0.8724453840732911\n",
    "{'C': 2, 'degree': 2, 'gamma': 2}\n",
    "AUC 0.8125440451021847\n",
    "RECALL 0.8372093023255814\n",
    "[[26  7]\n",
    " [ 7 36]]\n",
    "\n",
    "\n",
    "\n",
    " LogisticRegression\n",
    "TRAIN 0.9259953161592507\n",
    "TEST 0.8618745595489782\n",
    "{'C': 5, 'penalty': 'l1', 'solver': 'saga'}\n",
    "AUC 0.8090204369274137\n",
    "RECALL 0.8604651162790697\n",
    "[[25  8]\n",
    " [ 6 37]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7894736842105263"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('heart.csv')\n",
    "df = df.loc[:, ['cp','exang','thalach','sex','age','target']]\n",
    "\n",
    "random_state = 0\n",
    "\n",
    "X, y = df.drop(columns=['target']), df.loc[:, 'target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=random_state)\n",
    "\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "clf = SVC(random_state=random_state, C=10, gamma=0.1, degree=2).fit(X_train_scaled, y_train)\n",
    "clf.predict(X_test_scaled)\n",
    "clf.score(X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[24,  9],\n",
       "       [ 7, 36]])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, clf.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "clf.predict(scaler.transform([[1,1,180,1,50]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}